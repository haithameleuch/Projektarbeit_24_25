{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e6aaef",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838e6275",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import precision_score, f1_score, confusion_matrix, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from helper.earlyStop import EarlyStopping\n",
    "from helper.model import CNNModel\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66712035",
   "metadata": {},
   "source": [
    "# 1. Preprocessing to prepare data for training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f1ec7",
   "metadata": {},
   "source": [
    "### 1.1. Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d235ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and transform\n",
    "DATA_PATH = 'dataset/own_dataset/'\n",
    "BATCH_SIZE = 64\n",
    "RESULT_DIR = \"results\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4d92c",
   "metadata": {},
   "source": [
    "### 1.2. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee014d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),           # Convert to grayscale\n",
    "  #  transforms.RandomRotation(15),                         # Rotate images by Â±15 degrees\n",
    "   # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "   # transforms.RandomPerspective(distortion_scale=0.2, p=0.5), # Perspective transformation\n",
    "   # transforms.RandomHorizontalFlip(p=0.5),                # Random horizontal flip\n",
    "    transforms.Resize((28, 28)),                           # Resize to 28x28\n",
    "    transforms.ToTensor(),                                 # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))                   # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce4d1b",
   "metadata": {},
   "source": [
    "### 1.3. Load and Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95a6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(DATA_PATH, transform=data_transforms)\n",
    "\n",
    "# Calculate sizes of each partition\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = int(0.1 * total_size)\n",
    "eval_size = total_size - train_size - test_size  # To cover rounding#\n",
    "\n",
    "# Split\n",
    "train_dataset, test_dataset, eval_dataset = random_split(\n",
    "    dataset, [train_size, test_size, eval_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e2f8c",
   "metadata": {},
   "source": [
    "### 1.4. Create different Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e11496a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Get a batch from train_loader\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "labels.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9419ff",
   "metadata": {},
   "source": [
    "### 1.5.  Consider all of the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80e95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['air', 'earth', 'energy', 'fire', 'light', 'power','time', 'water' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f33ff2",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9251",
   "metadata": {},
   "source": [
    "### 2.1. Create model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628d7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dccf6",
   "metadata": {},
   "source": [
    "### 2.2. Set Configuration Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b2dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=1e-4)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "train_precision_list = []\n",
    "train_f1_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "val_precision_list = []\n",
    "val_f1_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214684a",
   "metadata": {},
   "source": [
    "### 2.3. Set MLFlow and train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4fac998",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_DIR = \"mlrun\"\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(MLFLOW_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a9fe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 0.5866 | Val Loss: 0.0684 Train Acc: 83.52% | Val Acc: 98.50% Val Precision: 0.9851 | Val F1: 0.9841\n",
      "Epoch [2/10] Train Loss: 0.0576 | Val Loss: 0.0203 Train Acc: 98.69% | Val Acc: 99.75% Val Precision: 0.9968 | Val F1: 0.9968\n",
      "Epoch [3/10] Train Loss: 0.0327 | Val Loss: 0.0227 Train Acc: 99.28% | Val Acc: 99.25% Val Precision: 0.9922 | Val F1: 0.9929\n",
      "Epoch [4/10] Train Loss: 0.0166 | Val Loss: 0.0053 Train Acc: 99.50% | Val Acc: 100.00% Val Precision: 1.0000 | Val F1: 1.0000\n",
      "Epoch [5/10] Train Loss: 0.0083 | Val Loss: 0.0193 Train Acc: 99.84% | Val Acc: 99.25% Val Precision: 0.9916 | Val F1: 0.9921\n",
      "Epoch [6/10] Train Loss: 0.0083 | Val Loss: 0.0049 Train Acc: 99.75% | Val Acc: 99.75% Val Precision: 0.9973 | Val F1: 0.9976\n",
      "Epoch [7/10] Train Loss: 0.0015 | Val Loss: 0.0022 Train Acc: 100.00% | Val Acc: 100.00% Val Precision: 1.0000 | Val F1: 1.0000\n",
      "Epoch [8/10] Train Loss: 0.0010 | Val Loss: 0.0023 Train Acc: 100.00% | Val Acc: 99.75% Val Precision: 0.9973 | Val F1: 0.9976\n",
      "Epoch [9/10] Train Loss: 0.0006 | Val Loss: 0.0009 Train Acc: 100.00% | Val Acc: 100.00% Val Precision: 1.0000 | Val F1: 1.0000\n",
      "Epoch [10/10] Train Loss: 0.0005 | Val Loss: 0.0018 Train Acc: 100.00% | Val Acc: 100.00% Val Precision: 1.0000 | Val F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(nested=True):\n",
    "    # Initialize lists to store probabilities for ROC calculation\n",
    "    all_train_probs = []\n",
    "    all_val_probs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "        epoch_train_probs = []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            epoch_train_probs.extend(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "\n",
    "        all_train_probs.append(epoch_train_probs)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_precision = precision_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "\n",
    "        train_loss_list.append(epoch_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_precision_list.append(train_precision)\n",
    "        train_f1_list.append(train_f1)\n",
    "\n",
    "        mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"train_precision\", train_precision, step=epoch)\n",
    "        mlflow.log_metric(\"train_f1_score\", train_f1, step=epoch)\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in eval_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "        all_val_probs.append(val_probs)\n",
    "        \n",
    "        val_loss = val_running_loss / len(eval_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_precision = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_precision_list.append(val_precision)\n",
    "        val_f1_list.append(val_f1)\n",
    "\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1_score\", val_f1, step=epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% \"\n",
    "              f\"Val Precision: {val_precision:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # === Plot individual accuracy metrics ===\n",
    "    def plot_single_metric(x_vals, y_vals, metric_name, color='blue', file_prefix=\"\"):\n",
    "        plt.figure()\n",
    "        plt.plot(x_vals, y_vals, color=color)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f\"{metric_name} per Epoch ({file_prefix})\")\n",
    "        plt.grid(True)\n",
    "        path = os.path.join(RESULT_DIR, f\"{file_prefix}_{metric_name.lower().replace(' ', '_')}.png\")\n",
    "        plt.savefig(path)\n",
    "        mlflow.log_artifact(path)\n",
    "        plt.close()\n",
    "\n",
    "    # Plot individual accuracy curves\n",
    "    plot_single_metric(range(1, len(train_acc_list)+1), train_acc_list, \"Accuracy\", 'blue', \"train\")\n",
    "    plot_single_metric(range(1, len(val_acc_list)+1), val_acc_list, \"Accuracy\", 'orange', \"val\")\n",
    "\n",
    "    # === Plot ROC curves ===\n",
    "    def plot_roc_curve(true_labels, probs, classes, title, file_prefix):\n",
    "        # Binarize the labels\n",
    "        y_true = label_binarize(true_labels, classes=classes)\n",
    "        n_classes = y_true.shape[1]\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), probs.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange', 'purple', 'brown']\n",
    "        \n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label='Class {0} (AUC = {1:0.2f})'\n",
    "                     ''.format(CATEGORIES[i], roc_auc[i]))\n",
    "        \n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average (AUC = {0:0.2f})'\n",
    "                 ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {title}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        path = os.path.join(RESULT_DIR, f\"{file_prefix}_roc_curve.png\")\n",
    "        plt.savefig(path)\n",
    "        mlflow.log_artifact(path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Log AUC metrics\n",
    "        for i in range(n_classes):\n",
    "            mlflow.log_metric(f\"{file_prefix}_auc_class_{CATEGORIES[i]}\", roc_auc[i])\n",
    "        mlflow.log_metric(f\"{file_prefix}_auc_micro_avg\", roc_auc[\"micro\"])\n",
    "\n",
    "    # Get the probabilities from the last epoch for ROC curves\n",
    "    last_train_probs = np.array(all_train_probs[-1])\n",
    "    last_val_probs = np.array(all_val_probs[-1])\n",
    "    \n",
    "    # Plot ROC curves for training and validation\n",
    "    plot_roc_curve(all_train_labels, last_train_probs, range(len(CATEGORIES)), \"Training\", \"train\")\n",
    "    plot_roc_curve(val_labels, last_val_probs, range(len(CATEGORIES)), \"Validation\", \"val\")\n",
    "\n",
    "    # === Save Best Model ===\n",
    "    best_model_path = \"cnn_model_best.pth\"\n",
    "    torch.save(early_stopping.best_model_state, best_model_path)\n",
    "    mlflow.log_artifact(best_model_path)\n",
    "    model.load_state_dict(early_stopping.best_model_state)\n",
    "\n",
    "    # === ONNX Export ===\n",
    "    onnx_model_path = \"cnn_model.onnx\"\n",
    "    dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(model, dummy_input, onnx_model_path,\n",
    "                      input_names=['input'], output_names=['output'],\n",
    "                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "                      opset_version=11)\n",
    "    mlflow.log_artifact(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a39b0",
   "metadata": {},
   "source": [
    "### 2.3. Evaluate the Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "740738d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for test_loader, dataset_name in [(test_loader, \"Dataset test\"), (eval_loader, \"Dataset evaluation\")]:\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    test_labels_np = np.array(test_labels)\n",
    "    test_preds_np = np.array(test_preds)\n",
    "    test_probs_np = np.array(test_probs)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_np, test_preds_np)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Reds\",\n",
    "                xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
    "    plt.title(f\"Confusion Matrix - {dataset_name}\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    mlflow.log_figure(plt.gcf(), f\"{dataset_name}_confusion_matrix.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defed944",
   "metadata": {},
   "source": [
    "# 4. Show the Results in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0466b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[2025-07-26 10:50:09 +0200] [109465] [INFO] Starting gunicorn 23.0.0\n",
      "[2025-07-26 10:50:09 +0200] [109465] [INFO] Listening at: http://127.0.0.1:5000 (109465)\n",
      "[2025-07-26 10:50:09 +0200] [109465] [INFO] Using worker: sync\n",
      "[2025-07-26 10:50:09 +0200] [109477] [INFO] Booting worker with pid: 109477\n",
      "[2025-07-26 10:50:09 +0200] [109478] [INFO] Booting worker with pid: 109478\n",
      "[2025-07-26 10:50:09 +0200] [109479] [INFO] Booting worker with pid: 109479\n",
      "[2025-07-26 10:50:09 +0200] [109480] [INFO] Booting worker with pid: 109480\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:488: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[2025-07-26 10:52:07 +0200] [109465] [INFO] Handling signal: int\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "[2025-07-26 10:52:07 +0200] [109478] [INFO] Worker exiting (pid: 109478)\n",
      "[2025-07-26 10:52:07 +0200] [109480] [INFO] Worker exiting (pid: 109480)\n",
      "[2025-07-26 10:52:07 +0200] [109479] [INFO] Worker exiting (pid: 109479)\n",
      "[2025-07-26 10:52:07 +0200] [109477] [INFO] Worker exiting (pid: 109477)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui --backend-store-uri mlrun\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
